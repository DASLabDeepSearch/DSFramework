# 国际新闻爬虫任务

动机：DeepSearch应用需要具体数据支撑，这些数据只能靠我们自己爬取，可以尽早做起来，而且最好能持续下去，得到的这批数据不管是对于项目还是对于后续其他的研究都是非常有价值的。

目标：每天爬取**新浪**和**搜狐**网站上的国际新闻，组织成结构化的数据存储在数据库中。每篇新闻需包含URL、原始HTML、标题、时间、来源、正文、文中图片URLs等字段信息。

分析：**新浪国际新闻**（https://news.sina.com.cn/world/）中“最新消息”板块会动态更新最新的新闻，通过不断下拉可以追溯以往的新闻（但最多追溯到一周前，如10月22日时最多只能追到10月16日的新闻，暂时未知是数量限制还是时间限制）。**搜狐国际新闻**（https://www.sohu.com/c/8/1461）中也是通过不断下拉获取以往新闻（最多也是一周前，10月22日时最多追到10月17日的新闻）。

技术选型：python写主体爬虫程序，使用Selenium模拟浏览器行为，拿到所有新闻URL后再抓取分析每篇新闻的数据，数据可存在MongoDB中。设置定时器，每天爬一次，注意不要重复爬（可通过新闻URL判重），学校网络每晚凌晨3点全部登出，需要重新输入学号密码登陆才能重新连外网，自动登录脚本见附录。建议设置日志和报警机制，当爬虫程序出错时可以通知开发人员进行维护。



附录：python3版自动登录脚本，根据需要自行修改测试

```python
import requests
import getpass
import time
import sched

url = "http://10.108.255.249/include/auth_action.php"
test_url = "http://www.baidu.com"

def connect():
	print("input uis username:")
	username = input()
	print("input uis password:")
	password = getpass.getpass()
	data = {
	    "action": "login",
	    "username": username,
	    "password": password,
	    "ac_id": "1",
	    "save_me": "0",
	    "ajax": "1",
	}
	r = requests.post(url, data)
	if r.status_code == 200:
		msg = "Login successfully!"
		try:
			test = requests.get(test_url, timeout=1)
			# print(test.text)
			if test.status_code == 200 and "baidu" in test.text:
				msg = msg + " Test successfully!"
			else:
				msg = "failed"
		except:
			msg = "failed"
	else:
		msg = "failed"
	print(time.strftime("%y-%m-%d %H:%M:%S") + " " + msg)

def run():
	s = sched.scheduler(time.time, time.sleep)
	s.enter(0, 0, connect)
	s.run()

def timer():
	while True:
		time.sleep(5)
		run()

if __name__ == "__main__":
	timer()
```

